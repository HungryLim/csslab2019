knitr::opts_chunk$set(echo = TRUE, root.dir = "~/dropbox/github/csslab2019/lab01/")
tweet_df[1:3, c("message", "label", "text")]
knitr::opts_chunk$set(echo = TRUE, root.dir = "~/dropbox/github/csslab2019/lab01/")
# labels of interest
tweet_df <- read.csv("Political-media-DFE.csv", stringsAsFactors = FALSE)
tweet_df <- tweet_df[tweet_df$message %in% c("policy", "personal", "information", "attack", "mobilization"), ]
tweet_df[1:3, c("message", "label", "text")]
sort(table(tweet_df$message), decreasing = TRUE)
setwd("C:/Users/wooki/Documents/GitHub/csslab2019/lab01")
tweet_df <- read.csv("Political-media-DFE.csv", stringsAsFactors = FALSE)
tweet_df <- tweet_df[tweet_df$message %in% c("policy", "personal", "information", "attack", "mobilization"), ]
head(tweat_df)
head(tweet_df)
tweet_df[1:3, c("message", "label", "text")]
install.packages("quanteda")
library(quanteda)
set.seed(139871)
choices <- c("lowercase", "rmdigits", "rmpunc", "stem", "rmstopwords", "rmbelowthreshold")
(group1 <- sample(choices, 3))
# "document feature" matrix
dtm <- quanteda::dfm(tweet_df$text, tolower = FALSE)
# so we can go about usual R business...
dtm_mat <- as.matrix(dtm)
# raw compared to preprocessed
tweet_df$text[1]
dtm_mat[1, dtm_mat[1,] != 0]
dtm_mat
dtm_mat[1]
dtm_mat[1,1]
dtm_mat[2]
dtm_mat[1, dtm_mat[1,] != 0]
dtm_mat[1,]
tokens(dtm_mat,remove_numbers)
# raw compared to preprocessed
tweet_df$text[1]
tokens(dtm_mat$text[1],remove_numbers)
tokens(tweet_df$text[1],remove_numbers)
tokens(tweet_df$text[1],remove_numbers=T)
tokens(tweet_df$text[1],remove_numbers=T,remove_symbols=T)
token <- tokenize(tweet_df$text[1], removePunct=TRUE, removeNumbers=TRUE)
token <- tokens(tweet_df$text[1], removePunct=TRUE, removeNumbers=TRUE)
tokens(tweet_df$text[1],remove_numbers=T,remove_symbols=T,remove_hyphens = T,remove_punct = TRUE)
tokens(tweet_df$text[2],remove_numbers=T,remove_symbols=T,remove_hyphens = T,remove_punct = TRUE)
tokens(tweet_df$text[3],remove_numbers=T,remove_symbols=T,remove_hyphens = T,remove_punct = TRUE)
tokens(tweet_df$text[3],remove_numbers=T,remove_symbols=T,remove_hyphens = T,remove_punct = TRUE)
tokens(tweet_df$text[4],remove_numbers=T,remove_symbols=T,remove_hyphens = T,remove_punct = TRUE)
tokens(tweet_df$text[5],remove_numbers=T,remove_symbols=T,remove_hyphens = T,remove_punct = TRUE)
tokens(tweet_df$text[66],remove_numbers=T,remove_symbols=T,remove_hyphens = T,remove_punct = TRUE)
tokens(tweet_df$text[2],remove_numbers=T,remove_symbols=T,remove_hyphens = T,remove_punct = TRUE)
tokens(tweet_df$text,remove_numbers=T,remove_symbols=T,remove_hyphens = T,remove_punct = TRUE)
processed<-tokens(tweet_df$text,remove_numbers=T,remove_symbols=T,remove_hyphens = T,remove_punct = TRUE)
trainingset <- sample(1:nrow(tweet_df), nrow(tweet_df)/2)
testset <- c(1:nrow(tweet_df))[-trainingset]
mod1 <- quanteda::textmodel_nb(x = dtm[trainingset],
y = tweet_df$message[trainingset],
prior = "uniform")
mod1
#model
trainingset <- sample(1:nrow(processed), nrow(processed)/2)
p<-as.matrix(processsed)
p<-as.matrix(processed)
#model
trainingset <- sample(1:nrow(p), nrow(p)/2)
trainingset
testset <- c(1:nrow(p))[-trainingset]
# train model
mod1 <- quanteda::textmodel_nb(x = dtm[trainingset],
y = tweet_df$message[trainingset],
prior = "uniform")
# basic checks of predictive ability
preds <- predict(mod1, newdata = dtm[testset])
table(tweet_df$message[testset] == preds) / length(testset)
table(p$message[testset] == preds) / length(testset)
# basic checks of predictive ability
preds <- predict(mod1, newdata = dtm[testset])
table(tweet_df$message[testset] == preds) / length(testset)
mod1 <- quanteda::textmodel_nb(x = dtm[trainingset],
y = p$message[trainingset],
prior = "uniform")
# basic checks of predictive ability
preds <- predict(mod1, newdata = dtm[testset])
head(p)
dim(p)
#1 line
tokens(tweet_df$text[1],remove_numbers=T,remove_symbols=T,remove_hyphens = T,remove_punct = TRUE)
#1 line
tokens(tweet_df$text,remove_numbers=T,remove_symbols=T,remove_hyphens = T,remove_punct = TRUE)
sd<-dfm(processed)
dim(sd)
#model
trainingset <- sample(1:nrow(p), nrow(p)/2)
testset <- c(1:nrow(p))[-trainingset]
# train model
mod1 <- quanteda::textmodel_nb(x = dtm[trainingset],
y = tweet_df$message[trainingset],
prior = "uniform")
# basic checks of predictive ability
preds <- predict(mod1, newdata = dtm[testset])
table(tweet_df$message[testset] == preds) / length(testset)
set.seed(129374)
trainingset <- sample(1:nrow(tweet_df), nrow(tweet_df)/2)
testset <- c(1:nrow(tweet_df))[-trainingset]
# train model
mod1 <- quanteda::textmodel_nb(x = dtm[trainingset],
y = tweet_df$message[trainingset],
prior = "uniform")
# basic checks of predictive ability
preds <- predict(mod1, newdata = dtm[testset])
table(tweet_df$message[testset] == preds) / length(testset)
dim(sd)
setwd("C:/Users/wooki/Documents/GitHub/csslab2019/lab01")
tweet_df <- read.csv("Political-media-DFE.csv", stringsAsFactors = FALSE)
tweet_df <- tweet_df[tweet_df$message %in% c("policy", "personal", "information", "attack", "mobilization"), ]
head(tweet_df)
tweet_df[1:3, c("message", "label", "text")]
sort(table(tweet_df$message), decreasing = TRUE)
#install.packages("quanteda")
library(quanteda)
set.seed(139871)
# "document feature" matrix
dtm <- quanteda::dfm(tweet_df$text, tolower = FALSE)
# so we can go about usual R business...
dtm_mat <- as.matrix(dtm)
#1 line
tokens(tweet_df$text,remove_numbers=T,remove_symbols=T,remove_hyphens = T,remove_punct = TRUE)
#1 line
#tokens(tweet_df$text,remove_numbers=T,remove_symbols=T,remove_hyphens = T,remove_punct = TRUE)
dtm_mat[1, dtm_mat[1,] != 0]
# process
processed<-tokens(tweet_df$text,remove_numbers=T,remove_symbols=T,remove_hyphens = T,remove_punct = TRUE)
sd<-dfm(processed)
dim(sd)
# process
processed<-tokens(tweet_df$text,remove_numbers=T,remove_symbols=T,remove_hyphens = T,remove_punct = TRUE)
sd<-dfm(processed)
dim(sd)
set.seed(129374)
trainingset <- sample(1:nrow(tweet_df), nrow(tweet_df)/2)
testset <- c(1:nrow(tweet_df))[-trainingset]
# train model
mod1 <- quanteda::textmodel_nb(x = dtm[trainingset],
y = tweet_df$message[trainingset],
prior = "uniform")
# basic checks of predictive ability
preds <- predict(mod1, newdata = dtm[testset])
table(tweet_df$message[testset] == preds) / length(testset)
dim(sd)
# process
processed<-tokens(tweet_df$text,remove_numbers=T,remove_symbols=T,remove_hyphens = T,remove_punct = TRUE,stem=T)
sd<-dfm(processed)
# "document feature" matrix
dtm <- quanteda::dfm(tweet_df$text, tolower = FALSE,,remove_numbers=T,remove_symbols=T,remove_hyphens = T,remove_punct = TRUE,stem=T)
# "document feature" matrix
dtm <- quanteda::dfm(tweet_df$text, tolower = FALSE,remove_numbers=T,remove_symbols=T,remove_hyphens = T,remove_punct = TRUE,stem=T)
dim(dtm)
# process
processed<-tokens(tweet_df$text,remove_numbers=T,remove_symbols=T,remove_hyphens = T,remove_punct = TRUE,stem=T)
processed[1]
dtm[1]
# so we can go about usual R business...
dtm_mat <- as.matrix(dtm)
dtm_mat[1]
dtm_mat[1, dtm_mat[1,]]
#1 line
#tokens(tweet_df$text,remove_numbers=T,remove_symbols=T,remove_hyphens = T,remove_punct = TRUE)
dtm_mat[1, dtm_mat[1,] != 0]
