---
title: "Lab01"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, root.dir = "~/dropbox/github/csslab2019/lab01/")
```

## Data for today

FYI, nice list of NLP datasets: https://github.com/niderhoff/nlp-datasets

I picked "Classification of political social media" for today https://www.figure-eight.com/data-for-everyone/

Download it yourself or its in the repo

Of particular interest to us today is the hand-coded "message" label

```{r}
# labels of interest
tweet_df <- read.csv("Political-media-DFE.csv", stringsAsFactors = FALSE)
tweet_df <- tweet_df[tweet_df$message %in% c("policy", "personal", "information", "attack", "mobilization"), ]
```

```{r}
tweet_df[1:3, c("message", "label", "text")]
sort(table(tweet_df$message), decreasing = TRUE)
```




## Some tools in R

Disclaimer: tons of tools in R, Python, etc.

I just use what works for my given task!

Note: Despite there being tons of built in functions, I always end up writing a lot of code for custom cleaning, or I write a custom script from the start.

```{r}
# install.packages("stm")
# install.packages("quanteda")
# install.packages("tm")
```



## Your task


1. Complete as much of the cleaning as you have time to do
2. Create a DTM
3. Describe your DTM
    - dimensions, avg number of tokens/document, etc.
4. Report predictive accuracy using test-set 

An incomplete list of preprocessing steps to consider:

- encoding issue (beyond today's lab)
- remove/keep numbers
- to lower case or not
- remove punctuation (maybe keep question marks and exclamation marks, though)
- stem/don't stem
- keep/remove hashtags or mentions (keep the words, remove punctuation?) (maybe keep an indicator?)
- remove stopwords
- remove links (but keep an indicator for if tweet contained a link)
- any custom stopwords of concern?
- remove tokens below a given threshold?
- etc.
- ... and order matters!
    

    
```{r}
# Everyone remove links
# Everyone keep hashtags and mentions
set.seed(139871)
choices <- c("lowercase", "rmdigits", "rmpunc", "stem", "rmstopwords", "rmbelowthreshold")
(group1 <- sample(choices, 3)) #keep '?' and '!' as tokens
(group2 <- sample(choices, 3)) #keep indicator for if there was a link
(group3 <- sample(choices, 3)) #remove custom list of stopwords you think relevant
(group4 <- sample(choices, 3)) #remove hashtags and mentions
```



## The "no" preprocessing example

Note, though, I'm still making lots of decisions even when do minimal operations

```{r}
# "document feature" matrix
dtm <- quanteda::dfm(tweet_df$text, tolower = FALSE)

# so we can go about usual R business...
dtm_mat <- as.matrix(dtm)

# raw compared to preprocessed
tweet_df$text[1]
dtm_mat[1, dtm_mat[1,] != 0]
```


```{r}
# describe DTM
dim(dtm_mat)
summary(rowSums(dtm_mat)) #yikes!
```


```{r, message=FALSE}
# let's all use the same training/test set
set.seed(129374)
trainingset <- sample(1:nrow(tweet_df), nrow(tweet_df)/2)
testset <- c(1:nrow(tweet_df))[-trainingset]

# train model
mod1 <- quanteda::textmodel_nb(x = dtm[trainingset],
                               y = tweet_df$message[trainingset],
                               prior = "uniform")

# basic checks of predictive ability
preds <- predict(mod1, newdata = dtm[testset])
table(tweet_df$message[testset] == preds) / length(testset)
table(tweet_df$message[testset], preds) 
```




